{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "sys.path.append('../')\n",
    "#from models.util import jsma_symbolic\n",
    "import tensorflow as tf\n",
    "import keras.backend as K\n",
    "import numpy as np\n",
    "from keras.utils import np_utils\n",
    "\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "# Create TF session and set as Keras backend session\n",
    "gpu_options = tf.GPUOptions(allow_growth=False)\n",
    "sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))\n",
    "K.set_session(sess)\n",
    "#print \"Created TensorFlow session and set Keras backend.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sys import getsizeof\n",
    "import gc\n",
    "def load_data():\n",
    "    ben_data = np.load('../data/ben_matrix.npy')\n",
    "    mal_data = np.load('../data/mal_matrix.npy')\n",
    "    \n",
    "    ben_lab = np.zeros((ben_data.shape[0]), dtype=np.int8)\n",
    "    mal_lab = np.ones((mal_data.shape[0]), dtype=np.int8)\n",
    "    ben_lab = np_utils.to_categorical(ben_lab, 2)\n",
    "    mal_lab = np_utils.to_categorical(mal_lab, 2)\n",
    "    \n",
    "    print (ben_data.shape, mal_data.shape)\n",
    "    print (getsizeof(ben_data), getsizeof(mal_data), getsizeof(ben_lab), getsizeof(mal_lab))\n",
    "    \n",
    "    X = np.concatenate((ben_data, mal_data), axis=0)\n",
    "    Y = np.concatenate((ben_lab, mal_lab), axis=0)\n",
    "    del ben_data, mal_data, ben_lab, mal_lab\n",
    "    gc.collect()\n",
    "    \n",
    "    num_samples = X.shape[0]\n",
    "    \n",
    "    print (num_samples)\n",
    "    num_val = int(.05*num_samples)\n",
    "    num_test = int(.05*num_samples)\n",
    "    num_train = num_samples - num_val - num_test\n",
    "    \n",
    "    reshuffle_idx = np.random.choice(range(X.shape[0]), num_samples,replace=False)\n",
    "    X = X[reshuffle_idx]\n",
    "    Y = Y[reshuffle_idx]\n",
    "    \n",
    "    X_train = X[:num_train]\n",
    "    Y_train = Y[:num_train]\n",
    "    \n",
    "    X_val = X[num_train:num_train+num_val]\n",
    "    Y_val = Y[num_train:num_train+num_val]\n",
    "    \n",
    "    X_test = X[num_train+num_val:]\n",
    "    Y_test = Y[num_train+num_val:]\n",
    "    \n",
    "    print (X_train.shape, X_val.shape, X_test.shape)\n",
    "    print (Y_train.shape, Y_val.shape, Y_test.shape)\n",
    "    \n",
    "    return X_train, Y_train, X_val, Y_val, X_test, Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((123453, 545333), (5560, 545333))\n",
      "(67322994961, 3032051592, 112, 112)\n"
     ]
    }
   ],
   "source": [
    "X_train, Y_train, X_val, Y_val, X_test, Y_test = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Input, Dense, Dropout, Activation, Flatten\n",
    "\n",
    "def build_model():\n",
    "    layers = [\n",
    "        Dense(64, input_shape=(545333,)),\n",
    "        Activation('relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(128),\n",
    "        Activation('relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(64),\n",
    "        Activation('relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(2),\n",
    "        Activation('softmax')\n",
    "    ]\n",
    "    \n",
    "    \n",
    "    model = Sequential()\n",
    "    for layer in layers:\n",
    "        model.add(layer)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#model = build_model()\n",
    "#model.compile(loss='categorical_crossentropy',\n",
    "#              optimizer='adadelta',\n",
    "#              metrics=['accuracy'])\n",
    "#model.fit(\n",
    "#    X_train, \n",
    "#    Y_train,\n",
    "##    epochs=5,\n",
    " #   batch_size=128,\n",
    "#    validation_data=(X_val, Y_val),\n",
    "#    verbose=1,\n",
    "#    shuffle=True\n",
    "#)\n",
    "\n",
    "#model_json = model.to_json()\n",
    "#with open('../trained_models/drebin.json', \"w\") as json_file:\n",
    "#    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "#model.save_weights('../trained_models/drebin.weights')\n",
    "#print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model()\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adadelta',\n",
    "              metrics=['accuracy'])\n",
    "json_file = open('../trained_models/drebin.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "# load weights into new model\n",
    "model.load_weights('../trained_models/drebin.weights')\n",
    "print(\"Loaded model from disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### define model loss\n",
    "def model_loss(y, prediction):\n",
    "    op = prediction.op\n",
    "    if \"softmax\" in str(op).lower():\n",
    "        logits, = op.inputs\n",
    "        #print \"logit\"\n",
    "    else:\n",
    "        logits = prediction\n",
    "        #print \"model\"        \n",
    "    out = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels =y)\n",
    "    return out\n",
    "\n",
    "def FGSM(X, Y, model, sess, batch_size=1):\n",
    "    x = tf.placeholder(dtype= tf.float32, shape=(1,545333))\n",
    "    y_ = tf.placeholder(dtype = tf.float32, shape=(1,2))\n",
    "    y = model(x)\n",
    "    loss = model_loss(y_, y)\n",
    "    gradients = tf.gradients(loss,x)\n",
    "    \n",
    "    adv = tf.sign(gradients)\n",
    "    #Get all non negative indices\n",
    "    neg = tf.constant(-1, shape=(1, 545333),dtype=tf.float32)\n",
    "    pos_idx = tf.cast(tf.not_equal(adv, neg), dtype=tf.float32)\n",
    "    #Add 1 to all indices that will make the loss go up\n",
    "    x_adv_raw = x + pos_idx\n",
    "    #Clip feature vectors to be 0 or 1\n",
    "    x_adv = tf.mod(x_adv_raw, 2)\n",
    "    \n",
    "    nb_batches = int(np.ceil(len(X) / batch_size))\n",
    "    X_adv = np.zeros_like(X)\n",
    "    for i in range(nb_batches):\n",
    "        X_adv[i*batch_size:(i+1)*batch_size] = sess.run(\n",
    "            x_adv,\n",
    "            feed_dict={\n",
    "                x: X[i*batch_size:(i+1)*batch_size],\n",
    "                y_: Y[i*batch_size: (i+1)*batch_size],\n",
    "                K.learning_phase():0\n",
    "            }\n",
    "        )\n",
    "        \n",
    "    return X_adv   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_iters=100\n",
    "nb_classes = 2\n",
    "nb_features=545333\n",
    "increase = True\n",
    "theta=1\n",
    "gamma=.05\n",
    "clip_min=0.0\n",
    "clip_max=1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_print(tensor, transform=None):\n",
    "\n",
    "    # Insert a custom python operation into the graph that does nothing but print a tensors value \n",
    "    def print_tensor(x):\n",
    "        # x is typically a numpy array here so you could do anything you want with it,\n",
    "        # but adding a transformation of some kind usually makes the output more digestible\n",
    "        print(x if transform is None else transform(x))\n",
    "        return x\n",
    "    log_op = tf.py_func(print_tensor, [tensor], [tensor.dtype])[0]\n",
    "    with tf.control_dependencies([log_op]):\n",
    "        res = tf.identity(tensor)\n",
    "\n",
    "    # Return the given tensor\n",
    "    return res\n",
    "\n",
    "\n",
    "# Same loop variables as above\n",
    "def modify_input(x_in, domain_in, y_in):\n",
    "    preds = model(x_in)\n",
    "    preds_onehot = tf.one_hot(tf.argmax(preds, axis=1), depth=nb_classes)\n",
    "    \n",
    "    # create the Jacobian graph\n",
    "    list_derivatives = []\n",
    "    for class_ind in xrange(nb_classes):\n",
    "        derivatives = tf.gradients(preds[:, class_ind], x_in)\n",
    "        list_derivatives.append(derivatives[0])\n",
    "    grads = tf.reshape(tf.stack(list_derivatives),\n",
    "                           shape=[nb_classes, -1, nb_features])\n",
    "\n",
    "    # Compute the Jacobian components\n",
    "    # To help with the computation later, reshape the target_class\n",
    "    # and other_class to [nb_classes, -1, 1].\n",
    "    # The last dimention is added to allow broadcasting later.\n",
    "    target_class = tf.reshape(tf.transpose(y_in, perm=[1, 0]),\n",
    "                                  shape=[nb_classes, -1, 1])\n",
    "    other_classes = tf.cast(tf.not_equal(target_class, 1), tf.float32)\n",
    "\n",
    "    grads_target = tf.reduce_sum(grads * target_class, axis=0)\n",
    "    grads_other = tf.reduce_sum(grads * other_classes, axis=0)\n",
    "\n",
    "    # Remove the already-used input features from the search space\n",
    "    # Subtract 2 times the maximum value from those values so that\n",
    "    # they won't be picked later\n",
    "    increase_coef = (4 * int(increase) - 2) \\\n",
    "            * tf.cast(tf.equal(domain_in, 0), tf.float32)\n",
    "\n",
    "    target_tmp = grads_target\n",
    "    target_tmp -= increase_coef \\\n",
    "            * tf.reduce_max(tf.abs(grads_target), axis=1, keep_dims=True)\n",
    "\n",
    "    other_tmp = grads_other\n",
    "    other_tmp += increase_coef \\\n",
    "            * tf.reduce_max(tf.abs(grads_other), axis=1, keep_dims=True)\n",
    "\n",
    "    # Create a mask to only keep features that match conditions\n",
    "    if increase:\n",
    "        scores_mask = ((target_tmp > 0) & (other_tmp < 0))\n",
    "    else:\n",
    "        scores_mask = ((target_tmp < 0) & (other_tmp > 0))\n",
    "\n",
    "   \n",
    "    scores = tf.cast(scores_mask, tf.float32) * (-target_tmp * other_tmp)\n",
    "    # Extract the best pixel\n",
    "    best = tf.argmax(\n",
    "                    tf.reshape(scores, shape=[-1, nb_features]),\n",
    "                    axis=1)[0]\n",
    "   \n",
    "    p1_one_hot = tf.one_hot(best, depth=nb_features)\n",
    " \n",
    "    # Update the search domain\n",
    "    domain_out = domain_in - p1_one_hot\n",
    "\n",
    "    # Apply the modification to the images\n",
    "    if increase:\n",
    "        #with tf.control_dependencies([p1_one_hot]):\n",
    "        x_out = tf.minimum(clip_max, x_in + p1_one_hot)\n",
    "    else:\n",
    "        x_out = tf.maximum(clip_min, x_in - p1_one_hot)\n",
    "    \n",
    "    return x_out, domain_out, preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def jsma(X, Y, model, sess, batch_size=1):\n",
    "    x = tf.placeholder(dtype=tf.float32, shape=(1,545333))\n",
    "    y = tf.placeholder(dtype = tf.float32, shape=(1,2))\n",
    "    search_domain = tf.placeholder(dtype=tf.float32, shape=(1,545333))\n",
    "    \n",
    "    x_out, domain_out, preds = modify_input(x, search_domain, y)\n",
    "    \n",
    "    #Iterate over entire batch\n",
    "    nb_batches = int(np.ceil(len(X) / batch_size))\n",
    "    X_adv = np.zeros_like(X)\n",
    "    avg_iterations = 0\n",
    "    for i in range(nb_batches):\n",
    "        x_i = X[i*batch_size:(i+1)*batch_size]\n",
    "        y_ = Y[i*batch_size: (i+1)*batch_size]\n",
    "        d_i = (x_i != 1).astype(int)  \n",
    "        for j in range(max_iters): \n",
    "            feed_dict={\n",
    "                    x: x_i,\n",
    "                    y: y_, \n",
    "                    search_domain: d_i,\n",
    "                    K.learning_phase():0 \n",
    "            }\n",
    "            \n",
    "            #Get updated values for input and search domain\n",
    "            x_i, d_i, pred = sess.run([x_out, domain_out, preds], feed_dict=feed_dict)\n",
    "            if (np.argmax(y_) == np.argmax(pred)):\n",
    "                break\n",
    "        #Update return matrix\n",
    "        X_adv[i*batch_size:(i+1)*batch_size] = x_i\n",
    "\n",
    "\n",
    "    return X_adv \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_tar = (Y_test == 0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_jsma = jsma(X_test, y_tar, model, sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(X_jsma, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(X_test, Y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
