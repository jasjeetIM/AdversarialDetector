{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import sys, os,gc\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "sys.path.append('../')\n",
    "\n",
    "from models.neural_network import NeuralNetwork\n",
    "from models.cnn import CNN\n",
    "from models.util import *\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\"\n",
    "\n",
    "#Seed used for all calculations of training and test point indices \n",
    "SEED = 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Define params of model\n",
    "input_shape = (28,28,1)\n",
    "num_classes = 10\n",
    "eps=0.3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Model Params: 3330314\n",
      "Loaded model from disk\n"
     ]
    }
   ],
   "source": [
    "#Load model from disk\n",
    "model_name = 'MNIST'\n",
    "model_save_path = '../trained_models/' + model_name + '-model.json'\n",
    "weights_save_path = '../trained_models/' + model_name + 'weights'\n",
    "model = CNN(model_name=model_name, dataset='mnist', seed=SEED)\n",
    "print ('Total Model Params: %d' % model.num_params)\n",
    "model.load_model(model_save_path, weights_save_path) \n",
    "#epochs = 50\n",
    "#model.train(epochs=epochs)\n",
    "#model.save_model(model_save_path, weights_save_path)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6055/6055 [==============================] - 2s 306us/step\n",
      "Model Accuracy: 0.99306\n"
     ]
    }
   ],
   "source": [
    "#Model Accuracy\n",
    "print ('Model Accuracy: %.5f' % (model.model.evaluate(model.test_data, model.test_labels)[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Get training samples\n",
    "num_train_samples = 1000\n",
    "data_indices = model.gen_rand_indices(low=0, high=model.train_data.shape[0], seed=SEED, num_samples=num_train_samples)\n",
    "train_data = model.train_data[data_indices]\n",
    "train_data_labels = model.train_labels[data_indices]\n",
    "train_data_labels_int = np.argmax(train_data_labels, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_test_samples_per_class = 10\n",
    "num_test_samples = num_classes*num_test_samples_per_class\n",
    "\n",
    "#Generate test points\n",
    "test_indices = model.gen_rand_indices_all_classes(y=model.test_labels, seed=SEED, num_samples=num_test_samples_per_class)\n",
    "\n",
    "#Get Regular, Noisy, FGSM, BIM, and CW test points\n",
    "reg_data = model.test_data[test_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate Hessian Vector Product Matrix to be used for Influence\n",
    "hvp_matrix = np.zeros((reg_data.shape[0], model.num_params))\n",
    "for idx, img_lab in enumerate(zip(reg_data, model.test_labels[test_indices])):\n",
    "    inverse_hvp = model.get_inverse_hvp(img_lab[0], img_lab[1])\n",
    "    hvp_matrix[idx,:] = inverse_hvp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reg_preds = model.model.predict(reg_data.reshape(-1,*input_shape))\n",
    "reg_labels = preds_to_labels(reg_preds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get Gradients required for Similarity\n",
    "grads_train = model.get_gradients_wrt_params(train_data, train_data_labels)\n",
    "grads_reg = model.get_gradients_wrt_params(reg_data, reg_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Calculate Influence and Similarity Matrices\n",
    "inf_matrix = np.dot(hvp_matrix, grads_train.T)\n",
    "sim_matrix = np.dot(grads_reg, grads_train.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Cos Sim: 0.99984, Ratio of Points Parallel: 1.00000\n"
     ]
    }
   ],
   "source": [
    "#Calculate percentage of points for which H^{-1} acts as a scaling\n",
    "avg_cos = 0.0\n",
    "scaling_count = 0.0\n",
    "for i in range(num_test_samples ):\n",
    "    hvp_norm=np.linalg.norm(hvp_matrix[i])\n",
    "    reg_norm = np.linalg.norm(grads_reg[i])\n",
    "    if hvp_norm > 0 and reg_norm > 0:\n",
    "        a = hvp_matrix[i]/hvp_norm\n",
    "        b = grads_reg[i]/reg_norm\n",
    "        cos=np.dot(a,b)\n",
    "        avg_cos+=cos\n",
    "        #consder > 0.99 as parallel due to approx errors\n",
    "        if cos >= 0.99:\n",
    "             scaling_count+=1.0\n",
    "    else:\n",
    "        print ('Divide by zero error, skipping point ...')\n",
    "\n",
    "print ('Avg Cos Sim: %.5f, Ratio of Points Parallel: %.5f' % ((avg_cos/num_test_samples), (scaling_count/num_test_samples)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Scaling Const: 99.49869, Ratio of Same Arg Max: 0.97000\n"
     ]
    }
   ],
   "source": [
    "#Get Avg Scale Constant and get ratio of test points for which (max sim == max inf)\n",
    "scaling_const = 0.0\n",
    "arg_max = 0.0\n",
    "for i in range(num_test_samples):\n",
    "    if (np.argmax(sim_matrix[i]) == np.argmax(inf_matrix[i])):\n",
    "        arg_max+=1.0\n",
    "    for j in range(num_train_samples):\n",
    "        const = sim_matrix[i,j]/inf_matrix[i,j]\n",
    "        scaling_const+=const\n",
    "\n",
    "print ('Avg Scaling Const: %.5f, Ratio of Same Arg Max: %.5f' % ((scaling_const/(num_test_samples*num_train_samples)), (arg_max/num_test_samples)))   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
