{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import sys, os,gc\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "sys.path.append('../')\n",
    "\n",
    "from models.neural_network import NeuralNetwork\n",
    "from models.cnn import CNN\n",
    "from models.util import *\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\"\n",
    "\n",
    "#Seed used for all calculations of training and test point indices \n",
    "SEED = 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Define params of model\n",
    "input_shape = (28,28,1)\n",
    "num_classes = 10\n",
    "eps=0.3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Model Params: 3330314\n",
      "Loaded model from disk\n"
     ]
    }
   ],
   "source": [
    "#Load model from disk\n",
    "model_name = 'MNIST'\n",
    "model_save_path = '../trained_models/' + model_name + '-model.json'\n",
    "weights_save_path = '../trained_models/' + model_name + 'weights'\n",
    "model = CNN(model_name=model_name, dataset='mnist', seed=SEED)\n",
    "print ('Total Model Params: %d' % model.num_params)\n",
    "model.load_model(model_save_path, weights_save_path) \n",
    "#epochs = 50\n",
    "#model.train(epochs=epochs)\n",
    "#model.save_model(model_save_path, weights_save_path)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6055/6055 [==============================] - 2s 333us/step\n",
      "Model Accuracy: 0.99306\n"
     ]
    }
   ],
   "source": [
    "#Model Accuracy\n",
    "print ('Model Accuracy: %.5f' % (model.model.evaluate(model.test_data, model.test_labels)[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Get training samples\n",
    "num_train_samples = 100\n",
    "data_indices = model.gen_rand_indices(low=0, high=model.train_data.shape[0], seed=SEED, num_samples=num_train_samples)\n",
    "train_data = model.train_data[data_indices]\n",
    "train_data_labels = model.train_labels[data_indices]\n",
    "train_data_labels_int = np.argmax(train_data_labels, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_test_samples_per_class = 10\n",
    "num_test_samples = num_classes*num_test_samples_per_class\n",
    "\n",
    "#Generate test points\n",
    "test_indices = model.gen_rand_indices_all_classes(y=model.test_labels, seed=SEED, num_samples=num_test_samples_per_class)\n",
    "#Get Regular, Noisy, FGSM, BIM, and CW test points\n",
    "reg_data = model.test_data[test_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Calculate Hessian Vector Product Matrix to be used for Influence\n",
    "hvp_matrix_train = np.zeros((train_data.shape[0], model.num_params))\n",
    "for idx, img_lab in enumerate(zip(train_data, train_data_labels)):\n",
    "    inverse_hvp = model.get_inverse_hvp(img_lab[0], img_lab[1])\n",
    "    hvp_matrix_train[idx,:] = inverse_hvp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate Hessian Vector Product Matrix to be used for Influence\n",
    "hvp_matrix = np.zeros((reg_data.shape[0], model.num_params))\n",
    "for idx, img_lab in enumerate(zip(reg_data, model.test_labels[test_indices])):\n",
    "    inverse_hvp = model.get_inverse_hvp(img_lab[0], img_lab[1])\n",
    "    hvp_matrix[idx,:] = inverse_hvp\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reg_preds = model.model.predict(reg_data.reshape(-1,*input_shape))\n",
    "reg_labels = preds_to_labels(reg_preds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_preds = model.model.predict(train_data.reshape(-1,*input_shape))\n",
    "train_labels = preds_to_labels(train_preds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Get Gradients required for Similarity\n",
    "grads_train = model.get_gradients_wrt_params(train_data, train_data_labels)\n",
    "grads_reg = model.get_gradients_wrt_params(reg_data, reg_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Calculate Influence and Similarity Matrices\n",
    "inf_matrix = np.dot(hvp_matrix, grads_train.T)\n",
    "sim_matrix = np.dot(grads_reg, grads_train.T)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Cos Sim: 100.20286, Var: 1.24936, Median: 99.98809, Min: 99.04225, Max: 107.24023\n"
     ]
    }
   ],
   "source": [
    "#Calculate percentage of points for which H^{-1} acts as a scaling\n",
    "avg_cos = 0.0\n",
    "cos_sim = list()\n",
    "for i in range(num_train_samples ):\n",
    "    hvp_norm=np.linalg.norm(hvp_matrix_train[i])\n",
    "    train_norm = np.linalg.norm(grads_train[i])\n",
    "    if hvp_norm > 0 and train_norm > 0:\n",
    "        \n",
    "        cos= train_norm/hvp_norm\n",
    "        cos_sim.append(cos)\n",
    "        \n",
    "    else:\n",
    "        print ('Divide by zero error, skipping point ...')\n",
    "\n",
    "print ('Avg Cos Sim: %.5f, Var: %.5f, Median: %.5f, Min: %.5f, Max: %.5f' % (np.mean(cos_sim), np.var(cos_sim), np.median(cos_sim), np.min(cos_sim), np.max(cos_sim)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Cos Sim: 0.99982, Var: 0.00000, Median: 0.99986, Min: 0.99938, Max: 1.00000\n"
     ]
    }
   ],
   "source": [
    "#Calculate percentage of points for which H^{-1} acts as a scaling\n",
    "avg_cos = 0.0\n",
    "cos_sim = list()\n",
    "for i in range(num_train_samples ):\n",
    "    hvp_norm=np.linalg.norm(hvp_matrix_train[i])\n",
    "    train_norm = np.linalg.norm(grads_train[i])\n",
    "    if hvp_norm > 0 and train_norm > 0:\n",
    "        a = hvp_matrix_train[i]/hvp_norm\n",
    "        b = grads_train[i]/train_norm\n",
    "        cos=np.dot(a,b)\n",
    "        cos_sim.append(cos)\n",
    "        \n",
    "    else:\n",
    "        print ('Divide by zero error, skipping point ...')\n",
    "\n",
    "print ('Avg Cos Sim: %.5f, Var: %.5f, Median: %.5f, Min: %.5f, Max: %.5f' % (np.mean(cos_sim), np.var(cos_sim), np.median(cos_sim), np.min(cos_sim), np.max(cos_sim)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg: -712.20943, Var: 64765896.70499, Median: 102.34315, Min: -80785.10632, Max: 113.94821\n",
      "Avg: 101.31130, Var: 257.53641, Median: 99.66969, Min: 89.14463, Max: 260.03273\n",
      "Avg: 102.42616, Var: 124.60414, Median: 100.85628, Min: 92.85352, Max: 197.16914\n",
      "Avg: 101.60558, Var: 5.96115, Median: 101.54564, Min: 90.55167, Max: 117.91940\n",
      "Avg: 100.64522, Var: 26.70253, Median: 101.08815, Min: 53.37242, Max: 112.94924\n",
      "Avg: 109.36228, Var: 4167.50166, Median: 99.78024, Min: 86.38978, Max: 712.59593\n",
      "Avg: 101.59134, Var: 8.45746, Median: 101.20142, Min: 97.46986, Max: 124.68836\n",
      "Avg: 102.42713, Var: 75.57832, Median: 101.47824, Min: 96.86417, Max: 185.99220\n",
      "Avg: 102.53529, Var: 285.00415, Median: 101.49361, Min: 85.17654, Max: 268.03638\n",
      "Avg: 98.82884, Var: 6.27165, Median: 99.32780, Min: 84.17610, Max: 104.40746\n",
      "Avg: 102.21326, Var: 84.88200, Median: 101.12827, Min: 94.83953, Max: 191.57206\n",
      "Avg: 102.42573, Var: 11.10107, Median: 102.51864, Min: 88.64582, Max: 115.14527\n",
      "Avg: 102.14648, Var: 85.35567, Median: 101.10381, Min: 95.87846, Max: 192.31399\n",
      "Avg: 101.92426, Var: 1.51319, Median: 101.97151, Min: 98.36358, Max: 107.61993\n",
      "Avg: 101.36344, Var: 7.65098, Median: 101.33451, Min: 90.48177, Max: 116.54428\n",
      "Avg: 98.52062, Var: 524.03682, Median: 101.29929, Min: -118.67909, Max: 114.05541\n",
      "Avg: 99.72773, Var: 88.17933, Median: 98.95255, Min: 86.26796, Max: 181.26460\n",
      "Avg: 101.50397, Var: 8.39944, Median: 101.54872, Min: 90.32734, Max: 118.47790\n",
      "Avg: 101.09733, Var: 23.62569, Median: 100.48381, Min: 94.59766, Max: 144.57049\n",
      "Avg: 100.76077, Var: 393.80029, Median: 99.39227, Min: 39.33157, Max: 254.15542\n",
      "Avg: 101.71031, Var: 92.36275, Median: 101.16830, Min: 82.42484, Max: 190.24612\n",
      "Avg: 101.25946, Var: 11.33298, Median: 101.63357, Min: 74.54993, Max: 107.39292\n",
      "Avg: 101.46170, Var: 27.47195, Median: 102.03821, Min: 71.50918, Max: 113.83000\n",
      "Avg: 101.87439, Var: 8.68730, Median: 101.40408, Min: 96.61673, Max: 114.23994\n",
      "Avg: 100.78126, Var: 37.85336, Median: 101.73779, Min: 50.11887, Max: 112.67860\n",
      "Avg: 98.71187, Var: 169.40079, Median: 99.22646, Min: 18.46276, Max: 144.92249\n",
      "Avg: 101.68853, Var: 88.19773, Median: 100.83429, Min: 69.54894, Max: 135.46897\n",
      "Avg: 101.27812, Var: 5.70493, Median: 101.24573, Min: 89.23152, Max: 111.90848\n",
      "Avg: 104.68173, Var: 396.19208, Median: 101.44563, Min: 97.41608, Max: 282.56904\n",
      "Avg: 107.02666, Var: 1779.77489, Median: 102.54238, Min: -85.87888, Max: 351.33681\n",
      "Avg: 100.70039, Var: 7.04757, Median: 100.81238, Min: 84.70491, Max: 110.73626\n",
      "Avg: 100.28997, Var: 39.62971, Median: 101.12785, Min: 39.71898, Max: 104.42725\n",
      "Avg: 104.30676, Var: 47.06531, Median: 103.97707, Min: 87.37078, Max: 137.13205\n",
      "Avg: 99.37015, Var: 357.00603, Median: 101.10675, Min: -81.87387, Max: 137.95567\n",
      "Avg: 99.34650, Var: 261.47082, Median: 99.07205, Min: 36.60886, Max: 236.43775\n",
      "Avg: 101.87083, Var: 11.17995, Median: 101.68099, Min: 86.86614, Max: 122.45913\n",
      "Avg: 101.10754, Var: 50.75510, Median: 102.38109, Min: 40.54646, Max: 107.67595\n",
      "Avg: 101.20493, Var: 1.80774, Median: 100.96570, Min: 97.39164, Max: 107.63718\n",
      "Avg: 106.73775, Var: 2177.61430, Median: 102.18965, Min: 85.14245, Max: 569.36354\n",
      "Avg: 100.13950, Var: 1.07015, Median: 100.14003, Min: 96.81919, Max: 106.97062\n",
      "Avg: 99.85044, Var: 3.16617, Median: 99.83893, Min: 93.74245, Max: 114.01835\n",
      "Avg: 101.72910, Var: 4.15162, Median: 101.57808, Min: 94.90687, Max: 110.20894\n",
      "Avg: 101.15228, Var: 6.05874, Median: 100.97109, Min: 87.43918, Max: 112.75677\n",
      "Avg: 101.03223, Var: 99.96264, Median: 101.67514, Min: 10.02825, Max: 135.61216\n",
      "Avg: 101.25071, Var: 172.90730, Median: 101.56924, Min: -21.63965, Max: 125.78515\n",
      "Avg: 102.31631, Var: 669.10346, Median: 100.52498, Min: 1.11908, Max: 319.95142\n",
      "Avg: 100.77933, Var: 41.82316, Median: 100.48391, Min: 69.19577, Max: 153.64014\n",
      "Avg: 100.14787, Var: 61.49223, Median: 100.63454, Min: 27.87923, Max: 113.13174\n",
      "Avg: 102.29970, Var: 42.50439, Median: 101.62769, Min: 96.00859, Max: 162.42858\n",
      "Avg: 100.44504, Var: 1.33457, Median: 100.39050, Min: 96.86803, Max: 107.65781\n",
      "Avg: 101.63221, Var: 8.13224, Median: 101.61976, Min: 96.56529, Max: 122.62280\n",
      "Avg: 102.99457, Var: 46.46890, Median: 102.36584, Min: 87.14592, Max: 159.76995\n",
      "Avg: 101.19376, Var: 1.51534, Median: 101.23317, Min: 96.37716, Max: 105.12483\n",
      "Avg: 99.97117, Var: 3.46214, Median: 99.91787, Min: 96.28987, Max: 116.75845\n",
      "Avg: 98.88968, Var: 281.16730, Median: 99.19045, Min: -6.69202, Max: 152.88096\n",
      "Avg: 102.23503, Var: 16.13449, Median: 102.07776, Min: 90.66993, Max: 131.43575\n",
      "Avg: 99.82877, Var: 1.81601, Median: 100.04115, Min: 88.30792, Max: 102.15949\n",
      "Avg: 107.98883, Var: 3.99665, Median: 107.79116, Min: 100.31884, Max: 121.14588\n",
      "Avg: 101.78128, Var: 12.32931, Median: 101.99781, Min: 78.35226, Max: 115.00465\n",
      "Avg: 101.25318, Var: 4.85821, Median: 100.96714, Min: 97.57201, Max: 116.16671\n",
      "Avg: 100.71373, Var: 15.87626, Median: 100.45210, Min: 94.17933, Max: 139.20380\n",
      "Avg: 90.43377, Var: 23222.56223, Median: 103.22102, Min: -1386.54484, Max: 418.63817\n",
      "Avg: 99.49481, Var: 12.77255, Median: 99.45091, Min: 74.80569, Max: 108.63792\n",
      "Avg: 101.50839, Var: 5.65139, Median: 101.29297, Min: 96.86330, Max: 116.69126\n",
      "Avg: 99.50413, Var: 40.36411, Median: 99.53989, Min: 76.66752, Max: 129.50660\n",
      "Avg: 101.33558, Var: 15.30228, Median: 101.24020, Min: 82.68796, Max: 130.29727\n",
      "Avg: 101.82085, Var: 82.93805, Median: 101.72851, Min: 36.39466, Max: 148.38717\n",
      "Avg: 101.48809, Var: 12.85994, Median: 101.28937, Min: 95.13969, Max: 133.93598\n",
      "Avg: 101.78352, Var: 2.94076, Median: 101.66664, Min: 98.53337, Max: 114.91017\n",
      "Avg: 101.22467, Var: 8.51887, Median: 101.00859, Min: 97.71358, Max: 127.89115\n",
      "Avg: 98.26639, Var: 250.08811, Median: 100.14555, Min: -18.37238, Max: 123.73938\n",
      "Avg: 102.02419, Var: 1.87274, Median: 102.15160, Min: 97.34097, Max: 106.37125\n",
      "Avg: 104.62937, Var: 1862.34010, Median: 101.82133, Min: -11.11991, Max: 518.66770\n",
      "Avg: 99.36506, Var: 44.15705, Median: 99.85915, Min: 37.99266, Max: 119.73135\n",
      "Avg: 100.35615, Var: 20.89430, Median: 100.83546, Min: 69.25303, Max: 105.63772\n",
      "Avg: 100.23995, Var: 10.73766, Median: 100.58296, Min: 69.87385, Max: 103.64449\n",
      "Avg: 98.17034, Var: 1553.33579, Median: 98.37362, Min: -111.60280, Max: 408.22302\n",
      "Avg: 100.48914, Var: 15.33948, Median: 101.02269, Min: 72.74624, Max: 105.21312\n",
      "Avg: 101.77349, Var: 51.80302, Median: 102.17912, Min: 42.32754, Max: 123.21644\n",
      "Avg: 99.91397, Var: 0.59422, Median: 99.97644, Min: 96.54692, Max: 103.48701\n",
      "Avg: 97.35097, Var: 530.29129, Median: 99.78970, Min: -130.26433, Max: 113.47502\n",
      "Avg: 102.42548, Var: 429.23030, Median: 100.44229, Min: 77.40080, Max: 303.35438\n",
      "Avg: 101.57645, Var: 75.22393, Median: 101.09339, Min: 73.19481, Max: 159.91080\n",
      "Avg: 100.97082, Var: 17.52979, Median: 101.53448, Min: 68.42813, Max: 106.52711\n",
      "Avg: 96.28276, Var: 2608.84582, Median: 101.87294, Min: -411.37165, Max: 104.43203\n",
      "Avg: 102.41387, Var: 63.11510, Median: 101.36403, Min: 94.89235, Max: 173.08035\n",
      "Avg: 100.15374, Var: 260.54486, Median: 100.85945, Min: -4.94261, Max: 181.06261\n",
      "Avg: 99.34920, Var: 3.67638, Median: 99.71284, Min: 84.12704, Max: 101.49411\n",
      "Avg: 103.18687, Var: 82.95496, Median: 102.15305, Min: 85.23833, Max: 170.60639\n",
      "Avg: 100.98492, Var: 29.93137, Median: 101.67938, Min: 59.53705, Max: 111.30850\n",
      "Avg: 101.19867, Var: 4.90021, Median: 101.11599, Min: 92.95558, Max: 109.56922\n",
      "Avg: 102.12412, Var: 2.30853, Median: 102.23968, Min: 96.93224, Max: 109.11508\n",
      "Avg: 100.60653, Var: 7.39752, Median: 101.15999, Min: 81.80019, Max: 104.36505\n",
      "Avg: 101.29291, Var: 15.74100, Median: 101.84115, Min: 69.55301, Max: 111.08958\n",
      "Avg: 102.57672, Var: 37.21892, Median: 102.13518, Min: 81.67138, Max: 155.93950\n",
      "Avg: 95.20546, Var: 1680.87500, Median: 99.22153, Min: -185.83240, Max: 184.44398\n",
      "Avg: 99.15874, Var: 65.11351, Median: 100.10539, Min: 25.76352, Max: 109.70822\n",
      "Avg: 101.88804, Var: 1.65225, Median: 101.88808, Min: 98.81141, Max: 109.45161\n",
      "Avg: 102.45040, Var: 38.48330, Median: 102.77463, Min: 49.13386, Max: 128.68470\n",
      "Avg: 101.23335, Var: 9.65883, Median: 101.74466, Min: 80.91924, Max: 108.57742\n"
     ]
    }
   ],
   "source": [
    "#Get Avg Scale Constant and get ratio of test points for which (max sim == max inf)\n",
    "scaling_const = 0.0\n",
    "arg_max = 0.0\n",
    "for j in range(num_train_samples):\n",
    "    #if (np.argmax(sim_matrix[i]) == np.argmax(inf_matrix[i])):\n",
    "    #    arg_max+=1.0\n",
    "    scaling = list()\n",
    "    for i in range(num_test_samples):\n",
    "        const = sim_matrix[i,j]/inf_matrix[i,j]\n",
    "        scaling.append(const)\n",
    "        scaling_const+=const\n",
    "    print ('Avg: %.5f, Var: %.5f, Median: %.5f, Min: %.5f, Max: %.5f' % (np.mean(scaling), np.var(scaling), np.median(scaling), np.min(scaling), np.max(scaling)))\n",
    "#print ('Avg Scaling Const: %.5f, Ratio of Same Arg Max: %.5f' % ((scaling_const/(num_test_samples*num_train_samples)), (arg_max/num_test_samples)))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg: 99.49839, Var: 139253.80111, Median: 100.92021, Min: -80902.50954, Max: 19914.82356\n"
     ]
    }
   ],
   "source": [
    "print ('Avg: %.5f, Var: %.5f, Median: %.5f, Min: %.5f, Max: %.5f' % (np.mean(scaling), np.var(scaling), np.median(scaling), np.min(scaling), np.max(scaling)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hvp = list()\n",
    "gt = list()\n",
    "for i in range(num_train_samples ):\n",
    "    hvp_norm=np.linalg.norm(hvp_matrix_train[i])\n",
    "    train_norm = np.linalg.norm(grads_train[i])\n",
    "    hvp.append(hvp_norm)\n",
    "    gt.append(train_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(hvp, gt, color='orange')\n",
    "plt.ylim(np.min(gt),np.median(gt))\n",
    "plt.xlim(np.min(hvp),np.median(hvp))\n",
    "plt.xlabel('Transformed', fontsize=12)\n",
    "plt.ylabel('Original', fontsize=12)\n",
    "plt.title('Scaling: Hessian Inverse')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.tight_layout()\n",
    "plt.savefig('hvp_gt.eps', format='eps', dpi=1000)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
